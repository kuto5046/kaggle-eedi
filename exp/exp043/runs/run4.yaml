# @package _global_
run_name: run4
notes: "lora_alpha=64 candidate=50 lr=1e-5"
use_fold: 0

retrieval_model:
  name: dunzhang/stella_en_1.5B_v5
  use_lora: true
  lora:
    r: 64
    lora_alpha: 64

max_candidates: 50
negative_size: 5
trainer:
  epoch: 10
  batch_size: 64
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  learning_rate: 1e-5
