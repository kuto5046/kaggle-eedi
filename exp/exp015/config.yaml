exp_name: exp015
run_name: run0  # 互換性を維持できてる場合インクリメントしても良い(parameterのみ変えて実行するときなど)
notes: "weight decay, lrをtuning"
tags: []

defaults:
  - _self_
  - path: local

# common
seed: 2024
debug: false
phase: train  # train, test
# cv
n_splits: 30  # hold-outで使うので実質fold=0しか使わない
use_fold: 0

feature_version: exp010

# unseenとして確保したquestionの何割をvalidに使うか。validの最終的なunseen_rateがこの値になるわけではない
# ここ大きくすると全trainのうち何割のmisconceptionがtrainに含まれるかが小さくなる
unseen_valid_rate: 0.6

retrieve_num: 25
num_candidates: 25  # 1つの学習データに紐づける負例の候補数

model:
  name: BAAI/bge-large-en-v1.5  # ここを変える場合data_processorを再実行する

trainer:
  epoch: 3
  batch_size: 16
  gradient_accumulation_steps: 32  # 128 // batch_size
  learning_rate: 2e-5
  weight_decay: 0.1
  warmup_ratio: 0.1
  # https://huggingface.co/docs/transformers/main/en/main_classes/optimizer_schedules#transformers.SchedulerType
  lr_scheduler_type: cosine_with_restarts
  save_strategy: steps
  save_steps: 0.2  # training_step全体の何%ごとに保存するか
  save_total_limit: 1  # 保存するcheckpointの最大数
  logging_strategy: steps
  logging_steps: 0.2  # training_step全体の何%ごとにログを出力するか
  eval_strategy: steps
  eval_steps: 0.2  # training_step全体の何%ごとに評価するか
  metric_for_best_model: eval_loss
  report_to: wandb


# ---------- Overriding hydra default configs ----------
hydra:
  job:
    chdir: false  # sub時はモジュール実行するため、chdirが有効化されない。そのためfalseにしておく
  run:
    dir: ${path.output_dir}/${exp_name}/${run_name}
  job_logging:
    root:
      level: INFO
    console:
      enabled: true
      format: "[%(asctime)s][%(name)s][%(levelname)s] - %(message)s"
      level: INFO
  searchpath:
      - file://conf
