###############
# experiment
###############
exp_name: exp048
run_name: run0  # 互換性を維持できてる場合インクリメントしても良い(parameterのみ変えて実行するときなど)
notes: "LLMをFT"
tags: []

defaults:
  - _self_
  - path: local

##############
# common
##############
seed: 2024
debug: false
phase: train  # train, test
# cv
n_splits: 5  # hold-outで使うので実質fold=0しか使わない
use_fold: 0

################
# data process
################
feature_version: ${exp_name}
retrieve_num: 25
max_candidates: 25  # 1つの学習データに紐づける負例の候補数
split_rate: 0.9

################
# model
################
retrieval_model:
  name: dunzhang/stella_en_1.5B_v5
  pretrained_path: ${path.model_dir}/exp046
  # アンサンブルで利用するモデルのパス
  names:
    - ${retrieval_model.name}
  weights:
    - 1.0
  use_lora: true
  lora:
    r: 64
    lora_alpha: 256
    lora_dropout: 0.05

llm_model:
  name: Qwen/Qwen2.5-32B-Instruct-AWQ
  use: true
  lora:
    r: 8
    lora_alpha: 16
    lora_dropout: 0.05

# 推論用vllm
vllm:
  model:
    model: ${llm_model.name}
    quantization: awq
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.90
    trust_remote_code: true
    dtype: half
    enforce_eager: true
    max_model_len: 3000
    disable_log_stats: true
    seed: ${seed}
    enable_lora: true  # LoRAの後付けを可能にする

  sampling:
    n: 1 # Number of output sequences to return for each prompt.
    top_p: 0.99  # Float that controls the cumulative probability of the top tokens to consider.
    temperature: 0.0  # randomness of the sampling
    skip_special_tokens: false  # Whether to skip special tokens in the output.
    max_tokens: 512  # Maximum number of tokens to generate per output sequence.
    seed: ${seed}


#############
# train
#############
negative_size: 1
shuffle_dataset: false
use_mask_pad_token: false

trainer:
  epoch: 1
  batch_size: 1
  gradient_accumulation_steps: 32
  gradient_checkpointing: true
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  # https://huggingface.co/docs/transformers/main/en/main_classes/optimizer_schedules#transformers.SchedulerType
  lr_scheduler_type: cosine
  save_strategy: steps  # custom callbackで手動保存する
  save_steps: 0.2  # training_step全体の何%ごとに保存するか
  save_total_limit: 1  # 保存するcheckpointの最大数
  logging_strategy: steps
  logging_steps: 0.2  # training_step全体の何%ごとにログを出力するか
  eval_strategy: steps
  eval_steps: 0.2  # training_step全体の何%ごとに評価するか
  metric_for_best_model: eval_loss
  report_to: wandb


# ---------- Overriding hydra default configs ----------
hydra:
  job:
    chdir: false  # sub時はモジュール実行するため、chdirが有効化されない。そのためfalseにしておく
  run:
    dir: ${path.output_dir}/${exp_name}/${run_name}
  job_logging:
    root:
      level: INFO
    console:
      enabled: true
      format: "[%(asctime)s][%(name)s][%(levelname)s] - %(message)s"
      level: INFO
  searchpath:
      - file://conf
